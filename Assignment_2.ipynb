{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae099ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_variance(data, ddof=0):\n",
    "    n = len(data)\n",
    "    mean = sum(data) / n\n",
    "    return sum((x - mean) ** 2 for x in data) / (n - ddof)\n",
    "# Note this is equivalent to np.var(Yt,ddof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d310f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_generate_cov(dim):\n",
    "    acc  = []\n",
    "    for i in range(dim):\n",
    "        row = np.ones((1,dim)) * corr\n",
    "        row[0][i] = 1\n",
    "        acc.append(row)\n",
    "    return np.concatenate(acc,axis=0)\n",
    "\n",
    "def fn_generate_multnorm(nobs,corr,nvar):\n",
    "\n",
    "    mu = np.zeros(nvar)\n",
    "    std = (np.abs(np.random.normal(loc = 1, scale = .5,size = (nvar,1))))**(1/2)\n",
    "    # generate random normal distribution\n",
    "    acc = []\n",
    "    for i in range(nvar):\n",
    "        acc.append(np.reshape(np.random.normal(mu[i],std[i],nobs),(nobs,-1)))\n",
    "    \n",
    "    normvars = np.concatenate(acc,axis=1)\n",
    "\n",
    "    cov = fn_generate_cov(nvar)\n",
    "    C = np.linalg.cholesky(cov)\n",
    "\n",
    "    Y = np.transpose(np.dot(C,np.transpose(normvars)))\n",
    "\n",
    "#     return (Y,np.round(np.corrcoef(Y,rowvar=False),2))\n",
    "    return Y\n",
    "\n",
    "def fn_randomize_treatment(N,p=0.5):\n",
    "    treated = random.sample(range(N), round(N*p))\n",
    "    return np.array([(1 if i in treated else 0) for i in range(N)]).reshape([N,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54fc8e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b1ab103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_generate_data(tau,N,p,p0,corr,conf = True,flagX = False):\n",
    "    \"\"\"\n",
    "    p0(int): number of covariates with nonzero coefficients\n",
    "    \"\"\"\n",
    "    nvar = p+2 # 1 confounder and variable for randomizing treatment\n",
    "    corr = 0.3 # correlation for multivariate normal\n",
    "\n",
    "    if conf==False:\n",
    "        conf_mult = 0 # remove confounder from outcome\n",
    "    if conf==True:\n",
    "        conf_mult= 1\n",
    "        \n",
    "    allX = fn_generate_multnorm(N,corr,nvar)\n",
    "    W0 = allX[:,0].reshape([N,1]) # variable for RDD assignment\n",
    "    C = allX[:,1].reshape([N,1]) # confounder\n",
    "    X = allX[:,2:] # observed covariates\n",
    "    \n",
    "    T = fn_randomize_treatment(N) # choose treated units\n",
    "    err = np.random.normal(0,1,[N,1])\n",
    "    beta0 = np.random.normal(5,5,[p,1])\n",
    "    \n",
    "    beta0[p0:p] = 0 # sparse model\n",
    "    Yab = tau*T+X@beta0+conf_mult*0.6*C+err\n",
    "    if flagX==False:\n",
    "        return (Yab,T)\n",
    "    else:\n",
    "        return (Yab,T,X)\n",
    "    \n",
    "    # regression discontinuity\n",
    "#     W = W0 + 0.5*C+3*X[:,80].reshape([N,1])-6*X[:,81].reshape([N,1])\n",
    "#     treated = 1*(W>0)\n",
    "#     Yrdd = 1.2* treated - 4*W + X@beta0 +0.6*C+err\n",
    "\n",
    "def fn_tauhat_means(Yt,Yc):\n",
    "    nt = len(Yt)\n",
    "    nc = len(Yc)\n",
    "    tauhat = np.mean(Yt)-np.mean(Yc)\n",
    "    se_tauhat = (np.var(Yt,ddof=1)/nt+np.var(Yc,ddof=1)/nc)**(1/2)\n",
    "    return (tauhat,se_tauhat)\n",
    "\n",
    "def fn_bias_rmse_size(theta0,thetahat,se_thetahat,cval = 1.96):\n",
    "    \"\"\"\n",
    "    theta0 - true parameter value\n",
    "    thetatahat - estimated parameter value\n",
    "    se_thetahat - estiamted se of thetahat\n",
    "    \"\"\"\n",
    "    b = thetahat - theta0\n",
    "    bias = np.mean(b)\n",
    "    rmse = np.sqrt(np.mean(b**2))\n",
    "    tval = b/se_thetahat # paramhat/se_paramhat H0: theta = 0\n",
    "    size = np.mean(1*(np.abs(tval)>cval))\n",
    "    # note size calculated at true parameter value\n",
    "    return (bias,rmse,size)\n",
    "\n",
    "def fn_run_experiments(tau,Nrange,p,p0,corr,conf,flagX=False):\n",
    "    n_values = []\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    lb = []\n",
    "    ub = []\n",
    "    for N in tqdm(Nrange):\n",
    "        n_values = n_values + [N]\n",
    "        if flagX==False:\n",
    "            Yexp,T = fn_generate_data(tau,N,p,p0,corr,conf,flagX)\n",
    "            Yt = Yexp[np.where(T==1)[0],:]\n",
    "            Yc = Yexp[np.where(T==0)[0],:]\n",
    "            tauhat,se_tauhat = fn_tauhat_means(Yt,Yc)            \n",
    "        elif flagX==1:\n",
    "            # use the right covariates in regression\n",
    "            Yexp,T,X = fn_generate_data(tau,N,p,p0,corr,conf,flagX)\n",
    "            Xobs = X[:,:p0]\n",
    "            covars = np.concatenate([T,Xobs],axis = 1)\n",
    "            mod = sm.OLS(Yexp,covars)\n",
    "            res = mod.fit()\n",
    "            tauhat = res.params[0]\n",
    "            se_tauhat = res.HC1_se[0]\n",
    "        elif flagX==2:\n",
    "            # use some of the right covariates and some \"wrong\" ones\n",
    "            Yexp,T,X = fn_generate_data(tau,N,p,p0,corr,conf,flagX)\n",
    "            Xobs1 = X[:,:np.int(p0/2)]\n",
    "            Xobs2 = X[:,-np.int(p0/2):]\n",
    "            covars = np.concatenate([T,Xobs1,Xobs2],axis = 1)\n",
    "            mod = sm.OLS(Yexp,covars)\n",
    "            res = mod.fit()\n",
    "            tauhat = res.params[0]\n",
    "            se_tauhat = res.HC1_se[0]\n",
    "            \n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]    \n",
    "        lb = lb + [tauhat-1.96*se_tauhat]\n",
    "        ub = ub + [tauhat+1.96*se_tauhat]\n",
    "        \n",
    "    return (n_values,tauhats,sehats,lb,ub)\n",
    "\n",
    "\n",
    "def fn_plot_with_ci(n_values,tauhats,tau,lb,ub,caption):\n",
    "    fig = plt.figure(figsize = (10,6))\n",
    "    plt.plot(n_values,tauhats,label = '$\\hat{\\\\tau}$')\n",
    "    plt.xlabel('N')\n",
    "    plt.ylabel('$\\hat{\\\\tau}$')\n",
    "    plt.axhline(y=tau, color='r', linestyle='-',linewidth=1,\n",
    "                label='True $\\\\tau$={}'.format(tau))\n",
    "    plt.title('{}'.format(caption))\n",
    "    plt.fill_between(n_values, lb, ub,\n",
    "        alpha=0.5, edgecolor='#FF9848', facecolor='#FF9848',label = '95% CI')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba64ceed",
   "metadata": {},
   "source": [
    "## 1. (a) Experiments with no covariates in the DGP (do not control any covariates)\n",
    "\n",
    "$y_i = \\tau*T_i+e_i$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5fbde784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 495/495 [00:01<00:00, 280.08it/s]\n"
     ]
    }
   ],
   "source": [
    "tau = 2 # treatment effect\n",
    "corr = .3 # corr between covariates\n",
    "conf=False # confounder\n",
    "p =10 # number of covariates generated\n",
    "p0 = 0 # number of covariates used in the DGP\n",
    "Nrange = range(10,1000,2) # loop over N values\n",
    "(nvalues,tauhats,sehats,lb,ub) = fn_run_experiments(tau,Nrange,p,p0,corr,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6aebe709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2000/2000 [00:01<00:00, 1186.51it/s]\n",
      "100%|██████████████████████████████████████| 2000/2000 [00:14<00:00, 136.07it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 2000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    for r in tqdm(range(R)):\n",
    "        Yexp,T = fn_generate_data(tau,N,10,0,corr,conf)\n",
    "        Yt = Yexp[np.where(T==1)[0],:]\n",
    "        Yc = Yexp[np.where(T==0)[0],:]\n",
    "        tauhat,se_tauhat = fn_tauhat_means(Yt,Yc)\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "acb608bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=-0.004756327866832036, RMSE=0.19584042722347184, size=0.0525\n",
      "N=1000: bias=-0.0007629977761247004, RMSE=0.0620269524783794, size=0.051\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518b54d",
   "metadata": {},
   "source": [
    "## 1.(b) Experiments with covariates in the DGP\n",
    "\n",
    "$y_i = \\tau*T_i+\\beta'*x_i+e_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e9e8ede8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 450/450 [00:01<00:00, 250.76it/s]\n"
     ]
    }
   ],
   "source": [
    "tau = 2\n",
    "corr = .3\n",
    "conf=False\n",
    "p = 10\n",
    "p0 = 3 # number of covariates used in the DGP\n",
    "Nrange = range(100,1000,2) # loop over N values\n",
    "# we need to start with more observations than p\n",
    "flagX = 1\n",
    "(nvalues1,tauhats1,sehats1,lb1,ub1) = fn_run_experiments(tau,Nrange,p,p0,corr,conf,flagX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19847bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2000/2000 [00:02<00:00, 670.74it/s]\n",
      "100%|██████████████████████████████████████| 2000/2000 [00:15<00:00, 126.17it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 2000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    for r in tqdm(range(R)):\n",
    "        Yexp,T,X = fn_generate_data(tau,N,10,3,corr,conf,flagX)\n",
    "        Xobs = X[:,:p0]\n",
    "        covars = np.concatenate([T,Xobs],axis = 1)\n",
    "        mod = sm.OLS(Yexp,covars)\n",
    "        res = mod.fit()\n",
    "        tauhat = res.params[0]\n",
    "        se_tauhat = res.HC1_se[0]\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f643e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=0.004316997640906438, RMSE=0.1436391319150795, size=0.0555\n",
      "N=1000: bias=0.00011584689422283634, RMSE=0.04452920913747773, size=0.05\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d0755",
   "metadata": {},
   "source": [
    "### Compare the results we get from a to b, we can see that when we control for covariates, the bias is getting smaller, RMSE goes down showing that we are getting better at estimating the treatment effect. The size of treatment effect is exactly at 5% when we test with 1000 sample size in (b), which is more precise than the size in (a). Since we test at 5%, the size of treatment effect is expected to be at 5%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79efeb9a",
   "metadata": {},
   "source": [
    "## DAG Example\n",
    "\n",
    "In this example, A directly affects dependent varibale E, at the same time, B, C and D are three other covariates that also influence the outcome of E.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cd48603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b639aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (20211204.2007)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"278pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 278.00 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-112 274,-112 274,4 -4,4\"/>\n",
       "<!-- B -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>B</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">B</text>\n",
       "</g>\n",
       "<!-- E -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>E</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"135\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"135\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">E</text>\n",
       "</g>\n",
       "<!-- B&#45;&gt;E -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>B&#45;&gt;E</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45.81,-76.81C63,-65.67 88.62,-49.06 107.99,-36.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.92,-39.43 116.4,-31.05 106.11,-33.56 109.92,-39.43\"/>\n",
       "</g>\n",
       "<!-- A -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>A</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">A</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;E -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>A&#45;&gt;E</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M107.35,-72.76C111.71,-64.28 117.15,-53.71 122.04,-44.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.23,-45.64 126.7,-35.15 119.01,-42.44 125.23,-45.64\"/>\n",
       "</g>\n",
       "<!-- C -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>C</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">C</text>\n",
       "</g>\n",
       "<!-- C&#45;&gt;E -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>C&#45;&gt;E</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M162.65,-72.76C158.29,-64.28 152.85,-53.71 147.96,-44.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"150.99,-42.44 143.3,-35.15 144.77,-45.64 150.99,-42.44\"/>\n",
       "</g>\n",
       "<!-- D -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>D</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"243\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"243\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">D</text>\n",
       "</g>\n",
       "<!-- D&#45;&gt;E -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>D&#45;&gt;E</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M224.19,-76.81C207,-65.67 181.38,-49.06 162.01,-36.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"163.89,-33.56 153.6,-31.05 160.08,-39.43 163.89,-33.56\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fc291d28700>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gr.Digraph()\n",
    "g.edge(\"B\", \"E\")\n",
    "g.edge(\"A\", \"E\")\n",
    "g.edge(\"C\", \"E\")\n",
    "g.edge(\"D\", \"E\")\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bb383b",
   "metadata": {},
   "source": [
    "## Real Life Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757da9a4",
   "metadata": {},
   "source": [
    "### For example, we randomly assign a number of plants (plants have same characteristics) into two groups; in treated group, we increase the nutrients in soil, in control group, we maintain the level of nutrients unchanged, then we compare the results from two groups to see how nutrients in soil affects the outcome of how the plant performs. In this case, the outcome of how a plant perfoms mainly depends on the level of nutrients in soil, however, other varibales such as the amount of light the plants can receive, the temperature, and the level of moisture can also affect the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26dfe0e",
   "metadata": {},
   "source": [
    "## 2. (a) Experiments that when failing to control for the confounder\n",
    "\n",
    " $y_i = \\tau*T_i+e_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "554bec45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 495/495 [00:01<00:00, 318.26it/s]\n"
     ]
    }
   ],
   "source": [
    "tau = 2 # treatment effect\n",
    "corr = 0.3 # corr between covariates\n",
    "conf= False # confounder\n",
    "p =10 # number of covariates generated\n",
    "p0 = 0 # number of covariates used in the DGP\n",
    "Nrange = range(10,1000,2) # loop over N values\n",
    "(nvalues2,tauhats2,sehats2,lb2,ub2) = fn_run_experiments(tau,Nrange,p,p0,corr,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "83ffc2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2000/2000 [00:02<00:00, 933.03it/s]\n",
      "100%|██████████████████████████████████████| 2000/2000 [00:14<00:00, 135.79it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 2000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    for r in tqdm(range(R)):\n",
    "        Yexp,T = fn_generate_data(tau,N,10,0,corr,conf)\n",
    "        Yt = Yexp[np.where(T==1)[0],:]\n",
    "        Yc = Yexp[np.where(T==0)[0],:]\n",
    "        tauhat,se_tauhat = fn_tauhat_means(Yt,Yc)\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "34a43de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=-0.008512082082271815, RMSE=0.20050150852120743, size=0.048\n",
      "N=1000: bias=0.0007853713722454434, RMSE=0.06350013909504715, size=0.053\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb94a16",
   "metadata": {},
   "source": [
    "## 2. (b) Experiments that when controlling for the confounder\n",
    "\n",
    "$y_i = \\tau*T_i+\\beta'*x_i+confmult*0.6*C+e_i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b32c5a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 450/450 [00:01<00:00, 260.73it/s]\n"
     ]
    }
   ],
   "source": [
    "tau = 2\n",
    "corr = .3\n",
    "conf=True\n",
    "p = 10\n",
    "p0 = 1 # number of covariates used in the DGP\n",
    "Nrange = range(100,1000,2) # loop over N values\n",
    "# we need to start with more observations than p\n",
    "flagX = 1\n",
    "(nvalues_2,tauhats_2,sehats_2,lb_2,ub_2) = fn_run_experiments(tau,Nrange,p,p0,corr,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "897a32b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2000/2000 [00:02<00:00, 718.28it/s]\n",
      "100%|██████████████████████████████████████| 2000/2000 [00:15<00:00, 129.17it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 2000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    for r in tqdm(range(R)): \n",
    "        Yexp,T,X = fn_generate_data(tau,N,10,1,corr,conf,flagX)\n",
    "        Xobs = X[:,:p0]\n",
    "        covars = np.concatenate([T,Xobs],axis = 1)\n",
    "        mod = sm.OLS(Yexp,covars)\n",
    "        res = mod.fit()\n",
    "        tauhat = res.params[0]\n",
    "        se_tauhat = res.HC1_se[0]\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2d0552ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=0.001961635568096632, RMSE=0.16042633542175405, size=0.0585\n",
      "N=1000: bias=0.00015584692015435864, RMSE=0.05196055200915378, size=0.0515\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584b9634",
   "metadata": {},
   "source": [
    "### Compare the results we get from a to b, we can see that when we control for the confounder, the bias is getting smaller, showing that we reduce chances of overestimating or underestimating the result, and RMSE goes down showing that we are getting better at estimating the treatment effect. The size of treatment effect is getting closer to 5% for size=1000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06032489",
   "metadata": {},
   "source": [
    "## DAG Example\n",
    "\n",
    "In this example, independent varibale A directly affects dependent varibale C, at the same time, B is a confounding variable that affect independent variable A as well as the outcome of C. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3411d04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (20211204.2007)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"89pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 89.00 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-184 85,-184 85,4 -4,4\"/>\n",
       "<!-- A -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>A</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">A</text>\n",
       "</g>\n",
       "<!-- C -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>C</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"54\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"54\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">C</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;C -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>A&#45;&gt;C</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M33.4,-72.41C36.51,-64.34 40.33,-54.43 43.83,-45.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47.13,-46.55 47.46,-35.96 40.6,-44.03 47.13,-46.55\"/>\n",
       "</g>\n",
       "<!-- B -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>B</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"54\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"54\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">B</text>\n",
       "</g>\n",
       "<!-- B&#45;&gt;A -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>B&#45;&gt;A</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M47.6,-144.41C44.49,-136.34 40.67,-126.43 37.17,-117.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"40.4,-116.03 33.54,-107.96 33.87,-118.55 40.4,-116.03\"/>\n",
       "</g>\n",
       "<!-- B&#45;&gt;C -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>B&#45;&gt;C</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M57.65,-143.91C59.68,-133.57 61.98,-120.09 63,-108 64.34,-92.06 64.34,-87.94 63,-72 62.28,-63.5 60.93,-54.31 59.49,-46.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.91,-45.29 57.65,-36.09 56.03,-46.56 62.91,-45.29\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fc28d804ee0>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gr.Digraph()\n",
    "g.edge(\"A\", \"C\")\n",
    "g.edge(\"B\", \"A\")\n",
    "g.edge(\"B\", \"C\")\n",
    "\n",
    "\n",
    "g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987996c",
   "metadata": {},
   "source": [
    "## Real Life Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590a84b",
   "metadata": {},
   "source": [
    "### For example, we observe that increased smoking leads to more disease; however, other variables such as  emotional stress could also cause smoking as well as disease. In this case, emotional stress is the confounder that both affect independent varible and dependent varible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc94a5",
   "metadata": {},
   "source": [
    "## 3. (a) Experiments that when controlling for the variable in between the path from cause to effect\n",
    "\n",
    "$y_i = \\tau*T_i+\\beta'*x_i+confmult*0.6*C+e_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f36e3486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 450/450 [00:01<00:00, 249.06it/s]\n"
     ]
    }
   ],
   "source": [
    "tau = 2\n",
    "corr = .3\n",
    "conf=True\n",
    "p = 10\n",
    "p0 = 3 # add two other variables that in between the path\n",
    "Nrange = range(100,1000,2) # loop over N values\n",
    "# we need to start with more observations than p\n",
    "flagX = 1\n",
    "(nvalues3,tauhats3,sehats3,lb3,ub3) = fn_run_experiments(tau,Nrange,p,p0,corr,conf,flagX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "070e4bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2000/2000 [00:03<00:00, 530.31it/s]\n",
      "100%|██████████████████████████████████████| 2000/2000 [00:19<00:00, 104.19it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 2000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    for r in tqdm(range(R)): \n",
    "        Yexp,T,X = fn_generate_data(tau,N,10,3,corr,conf,flagX)\n",
    "        Xobs = X[:,:p0]\n",
    "        covars = np.concatenate([T,Xobs],axis = 1)\n",
    "        mod = sm.OLS(Yexp,covars)\n",
    "        res = mod.fit()\n",
    "        tauhat = res.params[0]\n",
    "        se_tauhat = res.HC1_se[0]\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7a2e10b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=-0.0015467282599026242, RMSE=0.1635148765307156, size=0.058\n",
      "N=1000: bias=0.0013377122643154443, RMSE=0.050963541713635846, size=0.049\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ff868",
   "metadata": {},
   "source": [
    "## 3. (b) Experiments that when not controlling for the variable in between the path from cause to effect\n",
    "\n",
    "$y_i = \\tau*T_i+\\beta'*x_i+confmult*0.6*C+e_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1bd12575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 450/450 [00:03<00:00, 134.04it/s]\n"
     ]
    }
   ],
   "source": [
    "tau = 2\n",
    "corr = .3\n",
    "conf=True\n",
    "p = 10\n",
    "p0 = 1 # do not add varibales that in between the path\n",
    "Nrange = range(100,1000,2) # loop over N values\n",
    "# we need to start with more observations than p\n",
    "flagX = 1\n",
    "(nvalues_3,tauhats_3,sehats_3,lb_3,ub_3) = fn_run_experiments(tau,Nrange,p,p0,corr,conf,flagX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fbc0818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2000/2000 [00:05<00:00, 387.22it/s]\n",
      "100%|███████████████████████████████████████| 2000/2000 [00:27<00:00, 71.81it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 2000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    for r in tqdm(range(R)): \n",
    "        Yexp,T,X = fn_generate_data(tau,N,10,1,corr,conf,flagX)\n",
    "        Xobs = X[:,:p0]\n",
    "        covars = np.concatenate([T,Xobs],axis = 1)\n",
    "        mod = sm.OLS(Yexp,covars)\n",
    "        res = mod.fit()\n",
    "        tauhat = res.params[0]\n",
    "        se_tauhat = res.HC1_se[0]\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5c858c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=-0.0033521459411679833, RMSE=0.16404534519603345, size=0.055\n",
      "N=1000: bias=0.0002488916964088329, RMSE=0.05229208640279333, size=0.0535\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23793e5b",
   "metadata": {},
   "source": [
    "### Compare the results we get from a to b, we can see that when we not control for the variables in between the path from cause to effect, the bias is getting smaller, showing that we reduce chances of overestimating or underestimating the result; the RMSE and the size of treatment effect does not tell any significant difference. This indicates that we should not control for the variable in between the path, as those varibales are people select into."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece79c98",
   "metadata": {},
   "source": [
    "## DAG Example\n",
    "\n",
    "In this example, besides confonder varible B, there are two other varibles are in between the path from the cause A to effect C as well as from the cause B to effect C.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "04f73749",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (20211204.2007)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"184pt\" height=\"332pt\"\n",
       " viewBox=\"0.00 0.00 183.70 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-328 179.7,-328 179.7,4 -4,4\"/>\n",
       "<!-- A -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>A</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"93.7\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"93.7\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">A</text>\n",
       "</g>\n",
       "<!-- C -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>C</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"66.7\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"66.7\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">C</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;C -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>A&#45;&gt;C</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M91.54,-215.85C86.87,-178.83 75.81,-91.18 70.16,-46.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"73.6,-45.72 68.88,-36.23 66.66,-46.59 73.6,-45.72\"/>\n",
       "</g>\n",
       "<!-- D -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>D</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"38.7\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"38.7\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">D</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;D -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>A&#45;&gt;D</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M81.77,-217.81C74.49,-208.55 65.04,-196.52 56.85,-186.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"59.55,-183.86 50.62,-178.16 54.04,-188.18 59.55,-183.86\"/>\n",
       "</g>\n",
       "<!-- E -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>E</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"148.7\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"148.7\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">E</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;E -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>A&#45;&gt;E</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M100.12,-216.43C109.55,-192.09 127.25,-146.4 138.48,-117.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"141.81,-118.49 142.16,-107.9 135.28,-115.96 141.81,-118.49\"/>\n",
       "</g>\n",
       "<!-- B -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>B</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"65.7\" cy=\"-306\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"65.7\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">B</text>\n",
       "</g>\n",
       "<!-- B&#45;&gt;A -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>B&#45;&gt;A</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M72.34,-288.41C75.62,-280.22 79.65,-270.14 83.32,-260.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"86.65,-262.05 87.12,-251.47 80.15,-259.45 86.65,-262.05\"/>\n",
       "</g>\n",
       "<!-- B&#45;&gt;C -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>B&#45;&gt;C</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.98,-289.91C30.95,-261.92 -10.73,-199.73 2.7,-144 11.78,-106.38 34.2,-67.36 49.93,-43.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"52.98,-44.88 55.59,-34.61 47.15,-41 52.98,-44.88\"/>\n",
       "</g>\n",
       "<!-- B&#45;&gt;D -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>B&#45;&gt;D</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M62.45,-287.87C57.85,-263.67 49.39,-219.21 43.91,-190.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47.28,-189.36 41.97,-180.19 40.4,-190.67 47.28,-189.36\"/>\n",
       "</g>\n",
       "<!-- B&#45;&gt;E -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>B&#45;&gt;E</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M86.03,-293.92C100.77,-284.74 119.78,-270.25 129.7,-252 152.77,-209.61 153.46,-151.79 151.4,-118.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"154.89,-117.93 150.66,-108.22 147.91,-118.45 154.89,-117.93\"/>\n",
       "</g>\n",
       "<!-- D&#45;&gt;C -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>D&#45;&gt;C</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M42.08,-143.87C46.88,-119.56 55.7,-74.82 61.38,-46.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.81,-46.68 63.32,-36.19 57.95,-45.32 64.81,-46.68\"/>\n",
       "</g>\n",
       "<!-- D&#45;&gt;E -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>D&#45;&gt;E</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M57.87,-148.81C75.37,-137.67 101.46,-121.06 121.2,-108.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"123.21,-111.37 129.76,-103.05 119.45,-105.47 123.21,-111.37\"/>\n",
       "</g>\n",
       "<!-- E&#45;&gt;C -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>E&#45;&gt;C</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M132.5,-75.17C120.52,-64.94 104.01,-50.85 90.53,-39.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.47,-36.39 82.59,-32.56 87.93,-41.72 92.47,-36.39\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fc28da23880>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gr.Digraph()\n",
    "g.edge(\"A\", \"C\")\n",
    "g.edge(\"B\", \"A\")\n",
    "g.edge(\"B\", \"C\")\n",
    "g.edge(\"A\", \"D\")\n",
    "g.edge(\"D\", \"C\")\n",
    "g.edge(\"B\", \"D\")\n",
    "\n",
    "\n",
    "g.edge(\"A\", \"E\")\n",
    "g.edge(\"E\", \"C\")\n",
    "g.edge(\"B\", \"E\")\n",
    "g.edge(\"D\", \"E\")\n",
    "\n",
    "\n",
    "g\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437df7f5",
   "metadata": {},
   "source": [
    "## Real Life Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e44e7",
   "metadata": {},
   "source": [
    "### For example, we observe that advertisement leads to more sales of clothes, and Black Friday season is the confounding variable that can both leads to advertisement and sales of clothes; there could be two other variables that are in between the path from advertisement to sales of clothes as well as from Black Friday season to sales of clothes, such as \"watch the advertisement\" and then \"trust the advertisement\". These two varibles are in between the path from cause to effect that should not be controlled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
